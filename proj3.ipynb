{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9155b50-7ce5-4244-8c33-c81c285d0c89",
   "metadata": {},
   "source": [
    "Artificial Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "849de1e9-71c3-43cb-9915-f258d5f64f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 887us/step\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 862us/step\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 830us/step\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 957us/step\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 949us/step\n",
      "Mean RMSE across 5 folds: 0.836 ± 0.073\n",
      "Accuracy (within 0.5 of true rating): 0.615 ± 0.044\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Flatten, Dense, Dropout, Concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "def load_data(filepath):\n",
    "    r_cols = ['user_id', 'movie_id', 'rating', 'unix_timestamp']\n",
    "    return pd.read_csv(filepath, sep='\\t', names=r_cols, encoding='latin-1')\n",
    "\n",
    "def build_model(num_users, num_movies, embedding_size=50):\n",
    "    user_input = Input(shape=(1,), name='user_input')\n",
    "    user_embedding = Embedding(input_dim=num_users+1, output_dim=embedding_size, name='user_embedding')(user_input)\n",
    "    user_vec = Flatten(name='flatten_user')(user_embedding)\n",
    "\n",
    "    movie_input = Input(shape=(1,), name='movie_input')\n",
    "    movie_embedding = Embedding(input_dim=num_movies+1, output_dim=embedding_size, name='movie_embedding')(movie_input)\n",
    "    movie_vec = Flatten(name='flatten_movie')(movie_embedding)\n",
    "\n",
    "    concat = Concatenate()([user_vec, movie_vec])\n",
    "    dense = Dense(256, activation='relu')(concat)\n",
    "    batch_norm = BatchNormalization()(dense)\n",
    "    dropout = Dropout(0.5)(batch_norm)\n",
    "    output = Dense(1)(dropout)\n",
    "\n",
    "    model = Model(inputs=[user_input, movie_input], outputs=output)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0005), loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "def main():\n",
    "    # Determine the maximum number of users and movies to set the embedding input dimensions\n",
    "    ratings = pd.concat([load_data(f'ml-100k/u{k}.base') for k in range(1, 6)])\n",
    "    num_users = ratings['user_id'].max()\n",
    "    num_movies = ratings['movie_id'].max()\n",
    "\n",
    "    model = build_model(num_users, num_movies)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    rmses = []\n",
    "    accuracies = []\n",
    "\n",
    "    # Loop over each set of predefined splits\n",
    "    for k in range(1, 6):\n",
    "        train_data = load_data(f'ml-100k/u{k}.base')\n",
    "        test_data = load_data(f'ml-100k/u{k}.test')\n",
    "\n",
    "        # Preprocess ratings\n",
    "        y_train = scaler.fit_transform(train_data['rating'].values.reshape(-1, 1))\n",
    "        y_test = test_data['rating'].values\n",
    "\n",
    "        # Train model\n",
    "        model.fit([train_data['user_id'], train_data['movie_id']], y_train, \n",
    "                  batch_size=32, epochs=5, verbose=0)\n",
    "\n",
    "        # Predict and evaluate\n",
    "        preds = scaler.inverse_transform(model.predict([test_data['user_id'], test_data['movie_id']]))\n",
    "        rmse = sqrt(mean_squared_error(y_test, preds))\n",
    "        accuracy = np.mean(np.abs(preds.ravel() - y_test) <= 0.7)\n",
    "\n",
    "        rmses.append(rmse)\n",
    "        accuracies.append(accuracy)\n",
    "\n",
    "    print(f\"Mean RMSE across 5 folds: {np.mean(rmses):.3f} ± {np.std(rmses):.3f}\")\n",
    "    print(f\"Accuracy (within 0.5 of true rating): {np.mean(accuracies):.3f} ± {np.std(accuracies):.3f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6db9eb-d914-471b-9215-52b9035cb63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Gradient Boosting Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "25b633d9-0772-4946-b7c8-7780eb6488d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean RMSE across 5 folds: 0.895 ± 0.010\n",
      "Accuracy (within 0.5 of true rating): 0.433 ± 0.004\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "def load_data(base_path, test_path):\n",
    "    # Load base and test data\n",
    "    base_cols = ['user_id', 'movie_id', 'rating', 'timestamp']\n",
    "    test_cols = ['user_id', 'movie_id', 'rating', 'timestamp']\n",
    "\n",
    "    base_data = pd.read_csv(base_path, sep='\\t', names=base_cols)\n",
    "    test_data = pd.read_csv(test_path, sep='\\t', names=test_cols)\n",
    "\n",
    "    return base_data, test_data\n",
    "\n",
    "def prepare_features(data):\n",
    "    # Assuming that we have user and movie average ratings calculated\n",
    "    data['user_avg'] = data.groupby('user_id')['rating'].transform('mean')\n",
    "    data['movie_avg'] = data.groupby('movie_id')['rating'].transform('mean')\n",
    "    data['interaction'] = data['user_avg'] * data['movie_avg']\n",
    "\n",
    "    return data[['user_avg', 'movie_avg', 'interaction', 'rating']]\n",
    "\n",
    "def train_and_evaluate(train_data, test_data):\n",
    "    # Prepare features\n",
    "    X_train = train_data[['user_avg', 'movie_avg', 'interaction']]\n",
    "    y_train = train_data['rating']\n",
    "    X_test = test_data[['user_avg', 'movie_avg', 'interaction']]\n",
    "    y_test = test_data['rating']\n",
    "\n",
    "    # Train Gradient Boosting Regressor\n",
    "    model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "\n",
    "    # Calculate RMSE and accuracy\n",
    "    rmse = sqrt(mean_squared_error(y_test, preds))\n",
    "    accuracy = np.mean(np.abs(preds - y_test) <= 0.5)\n",
    "\n",
    "    return rmse, accuracy\n",
    "\n",
    "def main():\n",
    "    rmses = []\n",
    "    accuracies = []\n",
    "\n",
    "    # Iterate over each of the 5 predefined splits\n",
    "    for k in range(1, 6):\n",
    "        base_path = f'ml-100k/u{k}.base'\n",
    "        test_path = f'ml-100k/u{k}.test'\n",
    "        train_data, test_data = load_data(base_path, test_path)\n",
    "        \n",
    "        # Feature engineering\n",
    "        train_data = prepare_features(train_data)\n",
    "        test_data = prepare_features(test_data)\n",
    "        \n",
    "        rmse, accuracy = train_and_evaluate(train_data, test_data)\n",
    "        rmses.append(rmse)\n",
    "        accuracies.append(accuracy)\n",
    "    \n",
    "    # Output mean RMSE and Accuracy across 5 folds\n",
    "    print(f\"Mean RMSE across 5 folds: {np.mean(rmses):.3f} ± {np.std(rmses):.3f}\")\n",
    "    print(f\"Accuracy (within 0.5 of true rating): {np.mean(accuracies):.3f} ± {np.std(accuracies):.3f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adbdeb2-5ef1-46fd-bb8c-e95120d29380",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8628bd0d-b90e-463e-98c0-d4e55dd3770d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean RMSE across 5 folds: 1.107 ± 0.015\n",
      "Accuracy (within 0.5 of true rating): 0.334 ± 0.004\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "def load_data(filepath):\n",
    "    r_cols = ['user_id', 'movie_id', 'rating', 'unix_timestamp']\n",
    "    return pd.read_csv(filepath, sep='\\t', names=r_cols, encoding='latin-1')\n",
    "\n",
    "def load_item_data():\n",
    "    i_cols = ['movie_id', 'movie_title', 'release_date', 'video_release_date',\n",
    "              'IMDb_URL', 'unknown', 'Action', 'Adventure', 'Animation', \"Children's\",\n",
    "              'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror',\n",
    "              'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']\n",
    "    items = pd.read_csv('ml-100k/u.item', sep='|', names=i_cols, encoding='latin-1', usecols=range(24))\n",
    "    tfidf_transformer = TfidfTransformer(smooth_idf=True)\n",
    "    genre_matrix = items.iloc[:, 5:].fillna(0)\n",
    "    tfidf_features = tfidf_transformer.fit_transform(genre_matrix).toarray()\n",
    "    return items, tfidf_features\n",
    "\n",
    "def main():\n",
    "    items, items_tfidf = load_item_data()\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    rmses = []\n",
    "    accuracies = []\n",
    "\n",
    "    # 5-fold cross-validation process\n",
    "    for k in range(1, 6):  # For each of the 5 folds\n",
    "        train_path = f'ml-100k/u{k}.base'  # Correct file name for training data\n",
    "        test_path = f'ml-100k/u{k}.test'  # Correct file name for test data\n",
    "        train_data = load_data(train_path)\n",
    "        test_data = load_data(test_path)\n",
    "\n",
    "        # Merge TF-IDF features from the item data\n",
    "        train_data = train_data.join(items.set_index('movie_id'), on='movie_id')\n",
    "        test_data = test_data.join(items.set_index('movie_id'), on='movie_id')\n",
    "\n",
    "        # Prepare features and targets\n",
    "        X_train = items_tfidf[train_data['movie_id'] - 1]\n",
    "        X_test = items_tfidf[test_data['movie_id'] - 1]\n",
    "        y_train = scaler.fit_transform(train_data[['rating']])\n",
    "        y_test = test_data['rating']\n",
    "\n",
    "        # Model training\n",
    "        model = Ridge(alpha=0.01)\n",
    "        model.fit(X_train, y_train.ravel())\n",
    "\n",
    "        # Predict and evaluate\n",
    "        preds = scaler.inverse_transform(model.predict(X_test).reshape(-1, 1))\n",
    "        rmse = sqrt(mean_squared_error(y_test, preds))\n",
    "        rmses.append(rmse)\n",
    "        accuracy = np.mean(np.abs(preds.ravel() - y_test) <= 0.5)\n",
    "        accuracies.append(accuracy)\n",
    "\n",
    "    # Output results\n",
    "    print(f\"Mean RMSE across 5 folds: {np.mean(rmses):.3f} ± {np.std(rmses):.3f}\")\n",
    "    print(f\"Accuracy (within 0.5 of true rating): {np.mean(accuracies):.3f} ± {np.std(accuracies):.3f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "51675ec5-71ad-4b38-853f-ce0c8780f719",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Flatten, Dense, Dropout, Concatenate, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "def load_and_preprocess_data():\n",
    "    # Load Ratings\n",
    "    ratings = pd.read_csv('ml-100k/u.data', sep='\\t', names=['user_id', 'movie_id', 'rating', 'timestamp'], engine='python')\n",
    "    \n",
    "    # Load User Info\n",
    "    users = pd.read_csv('ml-100k/u.user', sep='|', names=['user_id', 'age', 'gender', 'occupation', 'zip_code'], engine='python')\n",
    "    \n",
    "    # Load Movie Info; ensure headers are correctly processed\n",
    "    movies = pd.read_csv('ml-100k/u.item', sep='|', encoding='latin-1', names=['movie_id', 'title', 'release_date', 'video_release_date', 'IMDb_URL', 'unknown', 'Action', 'Adventure', 'Animation', \"Children's\", 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror', 'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western'], usecols=range(24), engine='python')\n",
    "    \n",
    "    # Merge data\n",
    "    data = ratings.merge(users, on='user_id').merge(movies, on='movie_id')\n",
    "    \n",
    "    # Handling categorical data: One-hot encoding for 'gender' and 'occupation'\n",
    "    categorical_cols = ['gender', 'occupation']\n",
    "    data = pd.get_dummies(data, columns=categorical_cols)\n",
    "    \n",
    "    # Normalize numeric columns: 'age' and perhaps other continuous variables\n",
    "    scaler = StandardScaler()\n",
    "    numeric_cols = ['age']  # Add other numeric columns if needed\n",
    "    data[numeric_cols] = scaler.fit_transform(data[numeric_cols])\n",
    "    \n",
    "    return data\n",
    "\n",
    "# def main():\n",
    "#     data = load_and_preprocess_data()\n",
    "#     print(data.head())\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0fd5a516-766a-4991-b367-c18397e34e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 814us/step\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 819us/step\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 811us/step\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 818us/step\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 811us/step\n",
      "Final RMSE across 5 folds: 0.927 ± 0.006\n",
      "Accuracy (within ±0.5 of true rating): 0.425 ± 0.004\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Flatten, Dense, Dropout, Concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from math import sqrt\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Flatten, Dense, Dropout, Concatenate, Multiply, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def load_data(fold):\n",
    "    # Load training and testing data\n",
    "    train_data = pd.read_csv(f'ml-100k/u{fold}.base', sep='\\t', names=['user_id', 'movie_id', 'rating', 'timestamp'], engine='python')\n",
    "    test_data = pd.read_csv(f'ml-100k/u{fold}.test', sep='\\t', names=['user_id', 'movie_id', 'rating', 'timestamp'], engine='python')\n",
    "    \n",
    "    # Load user and item data\n",
    "    users = pd.read_csv('ml-100k/u.user', sep='|', names=['user_id', 'age', 'gender', 'occupation', 'zip_code'], engine='python')\n",
    "    items = pd.read_csv('ml-100k/u.item', sep='|', encoding='latin-1', names=['movie_id', 'movie_title', 'release_date', 'video_release_date', 'IMDb_URL', 'unknown', 'Action', 'Adventure', 'Animation', \"Children's\", 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror', 'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western'], usecols=range(24), engine='python')\n",
    "    \n",
    "    # Merge data\n",
    "    train_data = train_data.merge(users, on='user_id').merge(items, on='movie_id')\n",
    "    test_data = test_data.merge(users, on='user_id').merge(items, on='movie_id')\n",
    "    return train_data, test_data\n",
    "\n",
    "def preprocess_data(train_data, test_data):\n",
    "    # Ensure indices are reset to prevent InvalidIndexError during concatenation\n",
    "    train_data.reset_index(drop=True, inplace=True)\n",
    "    test_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Concatenate train and test data\n",
    "    combined = pd.concat([train_data, test_data], ignore_index=True)\n",
    "\n",
    "    # Encode categorical variables\n",
    "    encoder = OneHotEncoder(sparse_output=False)  # Outputs dense array directly\n",
    "    categorical_features = ['gender', 'occupation']  # Ensure these columns exist\n",
    "    categorical_encoded = encoder.fit_transform(combined[categorical_features])  # No need for .toarray()\n",
    "\n",
    "    # Create DataFrame for the encoded features\n",
    "    categorical_df = pd.DataFrame(categorical_encoded, columns=encoder.get_feature_names_out(), index=combined.index)\n",
    "\n",
    "    # Concatenate encoded features back to the combined DataFrame\n",
    "    combined = pd.concat([combined.drop(categorical_features, axis=1), categorical_df], axis=1)\n",
    "\n",
    "    # Scale numeric features\n",
    "    scaler = StandardScaler()\n",
    "    numeric_features = ['age']  # Ensure this column exists\n",
    "    combined[numeric_features] = scaler.fit_transform(combined[numeric_features])\n",
    "\n",
    "    # Separate back into train and test datasets based on the length of the original train data\n",
    "    train_processed = combined.iloc[:len(train_data)]\n",
    "    test_processed = combined.iloc[len(train_data):]\n",
    "    return train_processed, test_processed\n",
    "\n",
    "\n",
    "def build_advanced_model(num_users, num_movies, num_features):\n",
    "    user_input = Input(shape=(1,), dtype='int32')\n",
    "    movie_input = Input(shape=(1,), dtype='int32')\n",
    "    user_embedding = Embedding(num_users + 1, 30, embeddings_regularizer=l2(1e-5))(user_input)\n",
    "    movie_embedding = Embedding(num_movies + 1, 30, embeddings_regularizer=l2(1e-5))(movie_input)\n",
    "    \n",
    "    user_vec = Flatten()(user_embedding)\n",
    "    movie_vec = Flatten()(movie_embedding)\n",
    "    interact = Multiply()([user_vec, movie_vec])\n",
    "\n",
    "    additional_features_input = Input(shape=(num_features,), dtype='float32')\n",
    "    concat = Concatenate()([interact, additional_features_input])\n",
    "\n",
    "    dense1 = Dense(128, activation='relu')(concat)\n",
    "    batch_norm1 = BatchNormalization()(dense1)\n",
    "    dropout1 = Dropout(0.5)(batch_norm1)\n",
    "    dense2 = Dense(64, activation='relu')(dropout1)\n",
    "    batch_norm2 = BatchNormalization()(dense2)\n",
    "    dropout2 = Dropout(0.3)(batch_norm2)\n",
    "    output = Dense(1, activation='linear')(dropout2)\n",
    "\n",
    "    model = Model(inputs=[user_input, movie_input, additional_features_input], outputs=output)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "\n",
    "    return model\n",
    "\n",
    "def train_and_evaluate_model():\n",
    "    rmses = []\n",
    "    accuracies = []\n",
    "\n",
    "    for fold in range(1, 6):\n",
    "        train_data, test_data = load_data(fold)\n",
    "        train_processed, test_processed = preprocess_data(train_data, test_data)\n",
    "        num_users = max(train_processed['user_id'].max(), test_processed['user_id'].max())\n",
    "        num_movies = max(train_processed['movie_id'].max(), test_processed['movie_id'].max())\n",
    "\n",
    "        model = build_model(num_users, num_movies, train_processed.drop(['user_id', 'movie_id', 'rating', 'timestamp'], axis=1).shape[1])\n",
    "\n",
    "        model.fit([train_processed['user_id'], train_processed['movie_id']], train_processed['rating'], epochs=10, batch_size=32, verbose=0)\n",
    "\n",
    "        preds = model.predict([test_processed['user_id'], test_processed['movie_id']])\n",
    "        rmse = sqrt(mean_squared_error(test_processed['rating'], preds))\n",
    "        accuracy = np.mean(np.abs(preds.flatten() - test_processed['rating']) <= 0.5)\n",
    "\n",
    "        rmses.append(rmse)\n",
    "        accuracies.append(accuracy)\n",
    "\n",
    "    print(f\"Final RMSE across 5 folds: {np.mean(rmses):.3f} ± {np.std(rmses):.3f}\")\n",
    "    print(f\"Accuracy (within ±0.5 of true rating): {np.mean(accuracies):.3f} ± {np.std(accuracies):.3f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_and_evaluate_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "43fe93c0-4a43-4709-a701-e35c048bc34f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - loss: 1.0201\n",
      "Epoch 2/10\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 965us/step - loss: 0.9156\n",
      "Epoch 3/10\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - loss: 0.8974\n",
      "Epoch 4/10\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - loss: 0.8895\n",
      "Epoch 5/10\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - loss: 0.8849\n",
      "Epoch 6/10\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - loss: 0.8841\n",
      "Epoch 7/10\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - loss: 0.8722\n",
      "Epoch 8/10\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - loss: 0.8821\n",
      "Epoch 9/10\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - loss: 0.8728\n",
      "Epoch 10/10\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - loss: 0.8682\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
      "Epoch 1/10\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - loss: 1.0309\n",
      "Epoch 2/10\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - loss: 0.9266\n",
      "Epoch 3/10\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - loss: 0.9183\n",
      "Epoch 4/10\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - loss: 0.9031\n",
      "Epoch 5/10\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - loss: 0.8963\n",
      "Epoch 6/10\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - loss: 0.8905\n",
      "Epoch 7/10\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - loss: 0.8872\n",
      "Epoch 8/10\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - loss: 0.8886\n",
      "Epoch 9/10\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - loss: 0.8766\n",
      "Epoch 10/10\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - loss: 0.8741\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 943us/step\n",
      "Epoch 1/10\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - loss: 1.0293\n",
      "Epoch 2/10\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - loss: 0.9243\n",
      "Epoch 3/10\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - loss: 0.9093\n",
      "Epoch 4/10\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - loss: 0.9021\n",
      "Epoch 5/10\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - loss: 0.8962\n",
      "Epoch 6/10\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - loss: 0.8874\n",
      "Epoch 7/10\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - loss: 0.8908\n",
      "Epoch 8/10\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - loss: 0.8828\n",
      "Epoch 9/10\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - loss: 0.8754\n",
      "Epoch 10/10\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - loss: 0.8800\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 907us/step\n",
      "Epoch 1/10\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - loss: 1.0329\n",
      "Epoch 2/10\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - loss: 0.9217\n",
      "Epoch 3/10\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - loss: 0.9110\n",
      "Epoch 4/10\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - loss: 0.9041\n",
      "Epoch 5/10\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - loss: 0.8943\n",
      "Epoch 6/10\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - loss: 0.8907\n",
      "Epoch 7/10\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - loss: 0.8895\n",
      "Epoch 8/10\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - loss: 0.8948\n",
      "Epoch 9/10\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - loss: 0.8792\n",
      "Epoch 10/10\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - loss: 0.8779\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 983us/step\n",
      "Epoch 1/10\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - loss: 1.0397\n",
      "Epoch 2/10\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - loss: 0.9278\n",
      "Epoch 3/10\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - loss: 0.9050\n",
      "Epoch 4/10\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - loss: 0.9000\n",
      "Epoch 5/10\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - loss: 0.8984\n",
      "Epoch 6/10\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - loss: 0.8860\n",
      "Epoch 7/10\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - loss: 0.8949\n",
      "Epoch 8/10\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - loss: 0.8867\n",
      "Epoch 9/10\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - loss: 0.8773\n",
      "Epoch 10/10\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - loss: 0.8819\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 932us/step\n",
      "Final RMSE across 5 folds: 0.938 ± 0.009\n",
      "Accuracy (within ±0.5 of true rating): 0.433 ± 0.007\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Flatten, Dense, Dropout, Concatenate, Multiply, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from math import sqrt\n",
    "from tensorflow.keras.layers import LSTM, Embedding, Dense, Dropout, Flatten, Concatenate, Input, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def load_data(fold):\n",
    "    # Load training and testing data\n",
    "    train_data = pd.read_csv(f'ml-100k/u{fold}.base', sep='\\t', names=['user_id', 'movie_id', 'rating', 'timestamp'], engine='python')\n",
    "    test_data = pd.read_csv(f'ml-100k/u{fold}.test', sep='\\t', names=['user_id', 'movie_id', 'rating', 'timestamp'], engine='python')\n",
    "    \n",
    "    # Load user and item data\n",
    "    users = pd.read_csv('ml-100k/u.user', sep='|', names=['user_id', 'age', 'gender', 'occupation', 'zip_code'], engine='python')\n",
    "    items = pd.read_csv('ml-100k/u.item', sep='|', encoding='latin-1', names=['movie_id', 'movie_title', 'release_date', 'video_release_date', 'IMDb_URL', 'unknown', 'Action', 'Adventure', 'Animation', \"Children's\", 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror', 'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western'], usecols=range(24), engine='python')\n",
    "    \n",
    "    # Merge data\n",
    "    train_data = train_data.merge(users, on='user_id').merge(items, on='movie_id')\n",
    "    test_data = test_data.merge(users, on='user_id').merge(items, on='movie_id')\n",
    "    return train_data, test_data\n",
    "\n",
    "def preprocess_data(train_data, test_data, exclude_scaling):\n",
    "    train_data.reset_index(drop=True, inplace=True)\n",
    "    test_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    combined = pd.concat([train_data, test_data], ignore_index=True)\n",
    "\n",
    "    # One-hot encode categorical variables\n",
    "    categorical_features = ['gender', 'occupation']  # Adjust as needed\n",
    "    encoder = OneHotEncoder(sparse_output=False)\n",
    "    categorical_encoded = encoder.fit_transform(combined[categorical_features])\n",
    "    categorical_df = pd.DataFrame(categorical_encoded, columns=encoder.get_feature_names_out(), index=combined.index)\n",
    "\n",
    "    combined.drop(categorical_features, axis=1, inplace=True)\n",
    "    combined = pd.concat([combined, categorical_df], axis=1)\n",
    "\n",
    "    numeric_features = [col for col in combined.columns if combined[col].dtype in [np.float64, np.int64] and col not in exclude_scaling]\n",
    "    scaler = StandardScaler()\n",
    "    combined[numeric_features] = scaler.fit_transform(combined[numeric_features])\n",
    "\n",
    "    train_processed = combined.iloc[:len(train_data)]\n",
    "    test_processed = combined.iloc[len(train_data):]\n",
    "    return train_processed, test_processed\n",
    "\n",
    "def build_more_complex_model(num_users, num_movies, num_features):\n",
    "    # User and Movie embeddings\n",
    "    user_input = Input(shape=(1,), dtype='int32')\n",
    "    movie_input = Input(shape=(1,), dtype='int32')\n",
    "    user_embedding = Embedding(num_users + 1, 50, name=\"user_embedding\")(user_input)\n",
    "    movie_embedding = Embedding(num_movies + 1, 50, name=\"movie_embedding\")(movie_input)\n",
    "\n",
    "    # Flattening embedded layers\n",
    "    user_vec = Flatten()(user_embedding)\n",
    "    movie_vec = Flatten()(movie_embedding)\n",
    "\n",
    "    # Combine features with additional input features\n",
    "    additional_features_input = Input(shape=(num_features,), dtype='float32')\n",
    "    concat = Concatenate()([user_vec, movie_vec, additional_features_input])\n",
    "\n",
    "    # Neural network layers\n",
    "    dense = Dense(256, activation='relu')(concat)\n",
    "    batch_norm = BatchNormalization()(dense)\n",
    "    dropout = Dropout(0.5)(batch_norm)\n",
    "    final_layer = Dense(128, activation='relu')(dropout)\n",
    "    output = Dense(1, activation='linear')(final_layer)\n",
    "\n",
    "    model = Model(inputs=[user_input, movie_input, additional_features_input], outputs=output)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0001), loss='mean_squared_error')\n",
    "\n",
    "    return model\n",
    "\n",
    "def train_and_evaluate_model():\n",
    "    rmses = []\n",
    "    accuracies = []\n",
    "\n",
    "    # List of columns to exclude from feature input and scaling\n",
    "    exclude_scaling = ['movie_title', 'release_date', 'video_release_date', 'IMDb_URL', 'unknown', 'zip_code']\n",
    "\n",
    "    for fold in range(1, 6):\n",
    "        train_data, test_data = load_data(fold)\n",
    "        train_processed, test_processed = preprocess_data(train_data, test_data, exclude_scaling)\n",
    "        \n",
    "        num_users = train_processed['user_id'].nunique()\n",
    "        num_movies = train_processed['movie_id'].nunique()\n",
    "        feature_cols = [col for col in train_processed.columns if col not in ['user_id', 'movie_id', 'rating', 'timestamp'] + exclude_scaling]\n",
    "\n",
    "        model = build_advanced_model(num_users, num_movies, len(feature_cols))\n",
    "        model.fit([train_processed['user_id'], train_processed['movie_id'], train_processed[feature_cols]], train_processed['rating'], epochs=10, batch_size=32, verbose=1)\n",
    "\n",
    "        preds = model.predict([test_processed['user_id'], test_processed['movie_id'], test_processed[feature_cols]])\n",
    "        rmse = sqrt(mean_squared_error(test_processed['rating'], preds))\n",
    "        accuracy = np.mean(np.abs(preds.flatten() - test_processed['rating']) <= 0.5)\n",
    "\n",
    "        rmses.append(rmse)\n",
    "        accuracies.append(accuracy)\n",
    "\n",
    "    print(f\"Final RMSE across 5 folds: {np.mean(rmses):.3f} ± {np.std(rmses):.3f}\")\n",
    "    print(f\"Accuracy (within ±0.5 of true rating): {np.mean(accuracies):.3f} ± {np.std(accuracies):.3f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_and_evaluate_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5713f674-ea1e-4723-ae87-620ae4eeaa22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
